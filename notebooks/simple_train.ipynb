{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib, os\n",
    "\n",
    "data_path = '/ivi/ilps/personal/svakule/spoken_qa'\n",
    "\n",
    "# dataset_name = 'WD18_entities'\n",
    "dataset_name = 'WD18_relations'\n",
    "\n",
    "model_name = \"msmarco-distilbert-base-tas-b\"\n",
    "trained_on = 'original'\n",
    "\n",
    "#### Provide model save path\n",
    "model_save_path = os.path.join(\"/ivi/ilps/personal/svakule/msmarco\", \"output\", \"{}-{}-{}\".format(model_name, dataset_name, trained_on))\n",
    "os.makedirs(model_save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File /ivi/ilps/personal/svakule/spoken_qa/WD18_relations/train_original.jsonl not present! Please provide accurate file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c0412d7119db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mGenericDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqrels_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqrels_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_custom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqrels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrained_on\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-c0412d7119db>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(split, questions)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mqrels_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%s.tsv\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcorpus_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"corpus.jsonl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mGenericDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqrels_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqrels_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_custom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqrels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrained_on\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/beir/datasets/data_loader.py\u001b[0m in \u001b[0;36mload_custom\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfIn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"jsonl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfIn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"jsonl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfIn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqrels_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tsv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/beir/datasets/data_loader.py\u001b[0m in \u001b[0;36mcheck\u001b[0;34m(fIn, ext)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfIn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfIn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File {} not present! Please provide accurate file.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfIn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfIn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: File /ivi/ilps/personal/svakule/spoken_qa/WD18_relations/train_original.jsonl not present! Please provide accurate file."
     ]
    }
   ],
   "source": [
    "# load our dataset for training\n",
    "# KGQA dataset from https://github.com/askplatypus/wikidata-simplequestions\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "\n",
    "def load_data(split='valid', questions='original'):\n",
    "    query_path = os.path.join(data_path, dataset_name, \"%s_%s.jsonl\" % (split, questions))  # original text questions\n",
    "    # query_path = data_path + dataset + \"wav2vec2-base-960h.jsonl\"  # questions transcribed from synthethised speech\n",
    "    qrels_path = os.path.join(data_path, dataset_name, \"%s.tsv\" % split)\n",
    "    corpus_path = os.path.join(data_path, dataset_name, \"corpus.jsonl\")\n",
    "    return GenericDataLoader(corpus_file=corpus_path, query_file=query_path, qrels_file=qrels_path).load_custom()\n",
    "\n",
    "corpus, queries, qrels = load_data(split='train', questions=trained_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-18 21:02:10 - Load pretrained SentenceTransformer: msmarco-distilbert-base-tas-b\n",
      "2021-06-18 21:02:10 - Did not find folder msmarco-distilbert-base-tas-b\n",
      "2021-06-18 21:02:10 - Search model on server: http://sbert.net/models/msmarco-distilbert-base-tas-b.zip\n",
      "2021-06-18 21:02:10 - Load SentenceTransformer from folder: /home/svakule/.cache/torch/sentence_transformers/sbert.net_models_msmarco-distilbert-base-tas-b\n",
      "2021-06-18 21:02:11 - Use pytorch device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60b8ae6e4eb4436a79a8cba6ec9acb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Adding Input Examples', max=1420.0, style=ProgressStyle(dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-06-18 21:02:11 - Loaded 22719 training pairs.\n",
      "2021-06-18 21:02:11 - eval set contains 28497 documents and 2811 queries\n",
      "2021-06-18 21:02:11 - Starting to Train...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e06afad3b274e6c814e3f4bcd87ccdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caeaf2d78926425a83f23be8b1679ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1420.0, style=ProgressStyle(description_wâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-06-18 21:04:29 - Information Retrieval Evaluation on eval dataset after epoch 0:\n",
      "2021-06-18 21:04:38 - Queries: 2811\n",
      "2021-06-18 21:04:38 - Corpus: 28497\n",
      "\n",
      "2021-06-18 21:04:38 - Score-Function: cos_sim\n",
      "2021-06-18 21:04:38 - Accuracy@1: 95.30%\n",
      "2021-06-18 21:04:38 - Accuracy@3: 98.04%\n",
      "2021-06-18 21:04:38 - Accuracy@5: 98.26%\n",
      "2021-06-18 21:04:38 - Accuracy@10: 98.51%\n",
      "2021-06-18 21:04:38 - Precision@1: 95.30%\n",
      "2021-06-18 21:04:38 - Precision@3: 32.68%\n",
      "2021-06-18 21:04:38 - Precision@5: 19.65%\n",
      "2021-06-18 21:04:38 - Precision@10: 9.85%\n",
      "2021-06-18 21:04:38 - Recall@1: 95.30%\n",
      "2021-06-18 21:04:38 - Recall@3: 98.04%\n",
      "2021-06-18 21:04:38 - Recall@5: 98.26%\n",
      "2021-06-18 21:04:38 - Recall@10: 98.51%\n",
      "2021-06-18 21:04:38 - MRR@10: 0.9671\n",
      "2021-06-18 21:04:38 - NDCG@10: 0.9716\n",
      "2021-06-18 21:04:38 - MAP@100: 0.9673\n",
      "2021-06-18 21:04:38 - Score-Function: dot_score\n",
      "2021-06-18 21:04:38 - Accuracy@1: 95.70%\n",
      "2021-06-18 21:04:38 - Accuracy@3: 98.04%\n",
      "2021-06-18 21:04:38 - Accuracy@5: 98.22%\n",
      "2021-06-18 21:04:38 - Accuracy@10: 98.51%\n",
      "2021-06-18 21:04:38 - Precision@1: 95.70%\n",
      "2021-06-18 21:04:38 - Precision@3: 32.68%\n",
      "2021-06-18 21:04:38 - Precision@5: 19.64%\n",
      "2021-06-18 21:04:38 - Precision@10: 9.85%\n",
      "2021-06-18 21:04:38 - Recall@1: 95.70%\n",
      "2021-06-18 21:04:38 - Recall@3: 98.04%\n",
      "2021-06-18 21:04:38 - Recall@5: 98.22%\n",
      "2021-06-18 21:04:38 - Recall@10: 98.51%\n",
      "2021-06-18 21:04:38 - MRR@10: 0.9689\n",
      "2021-06-18 21:04:38 - NDCG@10: 0.9730\n",
      "2021-06-18 21:04:38 - MAP@100: 0.9691\n",
      "2021-06-18 21:04:38 - Save model to /ivi/ilps/personal/svakule/msmarco/output/msmarco-distilbert-base-tas-b-WD18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sample training script from https://github.com/UKPLab/beir/blob/main/examples/retrieval/training/train_sbert.py\n",
    "from sentence_transformers import losses, models, SentenceTransformer\n",
    "from beir import util, LoggingHandler\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.train import TrainRetriever\n",
    "\n",
    "#### Provide any sentence-transformers or HF model\n",
    "# model_name = \"distilbert-base-uncased\" \n",
    "# word_embedding_model = models.Transformer(model_name, max_seq_length=350)\n",
    "# pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "# model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "#### Or provide pretrained sentence-transformer model\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "retriever = TrainRetriever(model=model, batch_size=16)\n",
    "\n",
    "#### Prepare training samples\n",
    "train_samples = retriever.load_train(corpus, queries, qrels)\n",
    "train_dataloader = retriever.prepare_train(train_samples, shuffle=True)\n",
    "\n",
    "#### Training SBERT with cosine-product\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model=retriever.model)\n",
    "#### training SBERT with dot-product\n",
    "# train_loss = losses.MultipleNegativesRankingLoss(model=retriever.model, similarity_fct=util.dot_score)\n",
    "\n",
    "#### Prepare dev evaluator\n",
    "ir_evaluator = retriever.load_ir_evaluator(dev_corpus, dev_queries, dev_qrels)\n",
    "\n",
    "#### If no dev set is present from above use dummy evaluator\n",
    "# ir_evaluator = retriever.load_dummy_evaluator()\n",
    "\n",
    "#### Configure Train params\n",
    "num_epochs = 1\n",
    "evaluation_steps = 5000\n",
    "warmup_steps = int(len(train_samples) * num_epochs / retriever.batch_size * 0.1)\n",
    "\n",
    "retriever.fit(train_objectives=[(train_dataloader, train_loss)], \n",
    "                evaluator=ir_evaluator, \n",
    "                epochs=num_epochs,\n",
    "                output_path=model_save_path,\n",
    "                warmup_steps=warmup_steps,\n",
    "                evaluation_steps=evaluation_steps,\n",
    "                use_amp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
