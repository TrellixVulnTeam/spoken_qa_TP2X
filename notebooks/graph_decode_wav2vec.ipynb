{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE = 2  # pick sample from the dev set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function.Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  6, 10],\n",
      "        [ 0,  6, 10],\n",
      "        [ 0,  6, 10],\n",
      "        ...,\n",
      "        [ 0,  6,  4],\n",
      "        [ 0,  6,  4],\n",
      "        [ 0,  6,  4]])\n",
      "torch.Size([365, 3])\n",
      "what is a hume directed by vipcafon caronsfelt\n"
     ]
    }
   ],
   "source": [
    "# match entity label to the ctc table\n",
    "import os\n",
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "\n",
    "CTC_DEPTH = 3  # size of the ctc matrix considered for search\n",
    "NPATHS = 10 # number of longest paths on the bigram graph\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "tokenizer = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "\n",
    "path = \"../data/dev/\"\n",
    "\n",
    "file = str(SAMPLE) + '.wav'\n",
    "speech, samplerate = sf.read(path+file)\n",
    "i = int(file.split('.')[0]) - 1\n",
    "\n",
    "input_values = tokenizer(speech, return_tensors=\"pt\", padding=\"longest\").input_values\n",
    "logits = model(input_values).logits\n",
    "\n",
    "# find where s_tokens appear in the table\n",
    "ctc_table = torch.topk(logits, k=CTC_DEPTH, dim=-1)\n",
    "predicted_ids = ctc_table.indices[0]\n",
    "# predicted_ids = torch.argmax(logits, dim=-1).indices\n",
    "\n",
    "print(predicted_ids)\n",
    "print(predicted_ids.shape)\n",
    "\n",
    "# greedy decoding\n",
    "transcription = tokenizer.batch_decode(torch.argmax(logits, dim=-1))[0].lower()\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = np.transpose(np.array(predicted_ids))\n",
    "# print(predictions)\n",
    "# print(predictions.shape)\n",
    "# print(predictions[0])\n",
    "indices = predictions.flatten()\n",
    "# print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 365)\n",
      "[15.416867  15.432795  15.467757  ... -2.065003  -1.8960949 -1.6134164]\n"
     ]
    }
   ],
   "source": [
    "predictions_logits = np.transpose(ctc_table.values[0].detach().numpy())\n",
    "# print(predictions)\n",
    "print(predictions_logits.shape)\n",
    "# print(all_logits[0])\n",
    "indices_logits = predictions_logits.flatten()\n",
    "print(indices_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6107\n"
     ]
    }
   ],
   "source": [
    "# predictions = np.array([[2, 0, 0, 4, 5, 5],\n",
    "#                         [4, 5, 5, 6, 6, 1]])\n",
    "# generate adjacencies\n",
    "def connect(predictions, t, k, n):\n",
    "    edges = []\n",
    "    for j in range(predictions.shape[0]):  # offset\n",
    "        if predictions[j][k] != 0:\n",
    "            if predictions_logits[j][k] > 0:\n",
    "                edges.append([n*predictions.shape[1]+t, j*predictions.shape[1]+k])\n",
    "        else:\n",
    "            # skip to next if exists\n",
    "            if k < predictions.shape[1]-1:\n",
    "                edges.extend(connect(predictions, t, k+1, n))\n",
    "    return edges\n",
    "\n",
    "edges = []\n",
    "for t in range(predictions.shape[1]-1):  # columns\n",
    "    for n in range(predictions.shape[0]):  # rows\n",
    "        if predictions[n][t] != 0:\n",
    "            edges.extend(connect(predictions, t, t+1, n))\n",
    "\n",
    "print(len(edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357\n",
      "['TW', 'TH', 'TA', 'TE', 'IW', 'IH', 'IA', 'ED', 'IE', 'TI']\n"
     ]
    }
   ],
   "source": [
    "# TODO represent speech as bigrams with probs\n",
    "from collections import defaultdict\n",
    "\n",
    "special_tokens = [4]\n",
    "vocabulary = tokenizer.tokenizer.convert_ids_to_tokens\n",
    "\n",
    "bigrams = defaultdict(int)\n",
    "for e in edges:\n",
    "    i1 = indices[e[0]]\n",
    "    i2 = indices[e[1]]\n",
    "    # skip repeated predictions of the same letters (not a bigram)\n",
    "    if i1 != i2 and i1 not in special_tokens and i2 not in special_tokens:\n",
    "        bigrams[''.join(vocabulary([i1, i2]))] += indices_logits[e[0]] + indices_logits[e[1]]\n",
    "print(len(bigrams))\n",
    "\n",
    "print([k for k, v in sorted(bigrams.items(), key=lambda x: x[1], reverse=True)][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bottom up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28497 entity labels\n"
     ]
    }
   ],
   "source": [
    "# load entities\n",
    "import json\n",
    "\n",
    "path = '../data/'\n",
    "\n",
    "with open(path+'entities.json', 'r') as fin:\n",
    "    entities = json.load(fin)\n",
    "print(len(entities), 'entity labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is a film directed by wiebke von carolsfeld\n",
      "wiebke carolsfeld\n"
     ]
    }
   ],
   "source": [
    "# load original question\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"]'\n",
    "\n",
    "path = '../data/'\n",
    "with open(path+'annotated_wd_data_valid_answerable.txt') as fin:\n",
    "    lines = fin.readlines()\n",
    "    l = lines[SAMPLE-1]\n",
    "#         subject [tab] property [tab] object [tab] question\n",
    "    s, p, o, q = l.strip('\\n').split('\\t')\n",
    "    \n",
    "    q = re.sub(chars_to_ignore_regex, '', q).lower()\n",
    "    q = unidecode(q)\n",
    "    q = ''.join([j for i, j in enumerate(q) if j != q[i-1]])  # remove repeated letters\n",
    "    print(q)\n",
    "\n",
    "    s_label = entities[s]\n",
    "    s_label = re.sub(chars_to_ignore_regex, '', s_label).lower()\n",
    "    s_label = unidecode(s_label)\n",
    "    s_label = ''.join([j for i, j in enumerate(s_label) if j != s_label[i-1]])  # remove repeated letters\n",
    "    print(s_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate all bigrams\n",
    "from collections import Counter\n",
    "\n",
    "# get all bigrams\n",
    "bigrams = []\n",
    "for e in edges:\n",
    "    i1 = indices[e[0]]\n",
    "    i2 = indices[e[1]]\n",
    "    # skip repeated predictions of the same letters (not a bigram)\n",
    "    if i1 != i2 and i1 not in special_tokens and i2 not in special_tokens:\n",
    "        bigrams.append(''.join(vocabulary([i1, i2])).lower())  # normalise to lower case\n",
    "# count all bigrams frequency in ctc matrix\n",
    "bigrams = Counter(bigrams)\n",
    "        \n",
    "\n",
    "# special_tokens = [4]\n",
    "\n",
    "# vocabulary = tokenizer.tokenizer.convert_ids_to_tokens\n",
    "# def connect(predictions, t, k, n):\n",
    "#     edges = []\n",
    "#     for j in range(predictions.shape[0]):  # offset\n",
    "#         if predictions[j][k] != 0:# and predictions[j][k] != predictions[n][t]:\n",
    "#             # skip repeated predictions of the same letters (not a bigram)\n",
    "#             if predictions[n][t] != predictions[j][k]:\n",
    "#                 if predictions[n][t] not in special_tokens and predictions[j][k] not in special_tokens:\n",
    "#                     edges.append(''.join(vocabulary([predictions[n][t], predictions[j][k]])))\n",
    "#         else:\n",
    "# #             predictions[j][k] = 0\n",
    "#             # skip to next if exists\n",
    "#             if k < predictions.shape[1]-1:\n",
    "#                 edges.extend(connect(predictions, t, k+1, n))\n",
    "#     return edges\n",
    "\n",
    "# edges = []\n",
    "# for t in range(predictions.shape[1]-1):  # columns\n",
    "#     for n in range(predictions.shape[0]):  # rows\n",
    "#         if predictions[n][t] != 0:\n",
    "#             edges.extend(connect(predictions, t, t+1, n))\n",
    "\n",
    "# bigrams = Counter(edges)\n",
    "# print(len(edges), 'edges')\n",
    "# print(len(bigrams), 'bigrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 entity bigrams\n"
     ]
    }
   ],
   "source": [
    "# load all entity labels that match the top bigrams\n",
    "import json\n",
    "\n",
    "path = '../data/'\n",
    "\n",
    "with open(path+'bigrams_index.json', 'r') as fin:\n",
    "    entity_bigrams = json.load(fin)\n",
    "print(len(entity_bigrams), 'entity bigrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "# e_labels = Counter()\n",
    "# for b, c in bigrams.most_common(10):\n",
    "#     e_ids = entity_bigrams[b.lower()]\n",
    "#     for e_id in e_ids:\n",
    "#         e_label = list(entities.values())[e_id]\n",
    "#         e_labels[e_label] += 1\n",
    "# print(len(e_labels))\n",
    "\n",
    "# top_labels = [l for l, c in e_labels.items() if c > 1]\n",
    "# print(len(top_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ir\n",
      "ca\n",
      "ro\n",
      "21089\n"
     ]
    }
   ],
   "source": [
    "# bottom up from greedy decoding\n",
    "bottom_bigrams = set([word[i:i + 2] for word in transcription.lower().split() for i in range(0, len(word)-1, 1)])\n",
    "\n",
    "# for query in s_label.lower().split():\n",
    "#     print(query)\n",
    "#     for i in range(0, len(query)-1, 1):\n",
    "#         bigram = query[i:i + 2]\n",
    "#         if bigram in bottom_bigrams:\n",
    "#             print(bigram)\n",
    "            \n",
    "# match to the bigrams from the greedy decoded transcription\n",
    "e_labels = Counter()\n",
    "for b in bottom_bigrams:\n",
    "    matched_e_labels = entity_bigrams[b]\n",
    "    for e_label in matched_e_labels:\n",
    "        e_labels[e_label] += 1\n",
    "        if e_label == 'cairo':\n",
    "            print(b)\n",
    "print(len(e_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiebke\n",
      "carolsfeld\n",
      "ca\n",
      "ar\n",
      "ro\n",
      "sf\n",
      "fe\n",
      "el\n"
     ]
    }
   ],
   "source": [
    "# check entity matches top bigrams\n",
    "# top_bigrams = [k.lower() for k, v in sorted(bigrams.items(), key=lambda x: x[1], reverse=True)][:20]\n",
    "# [b for b, c in bigrams.most_common(10)]\n",
    "# print(top_bigrams)\n",
    "\n",
    "for query in s_label.lower().split():\n",
    "    print(query)\n",
    "    for i in range(0, len(query)-1, 1):\n",
    "        bigram = query[i:i + 2]\n",
    "        if bigram in bottom_bigrams:\n",
    "            print(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ir', 'by', 'te', 'at', 'ip', 'ha', 'sf', 'me', 'el', 'hu', 'lt', 'pc', 'ed', 'on', 'fe', 'fo', 'ns', 'um', 'ca', 'di', 'ro', 'af', 're', 'ct', 'is', 'vi', 'ar', 'wh', 'ec'}\n",
      "500\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "topn = 500\n",
    "\n",
    "print(bottom_bigrams)\n",
    "# print(e_labels.most_common(topn))\n",
    "\n",
    "\n",
    "top_labels = [l for l, c in e_labels.most_common(topn)]\n",
    "print(len(top_labels))\n",
    "\n",
    "print(s_label in top_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import itertools\n",
    "import difflib\n",
    "\n",
    "\n",
    "def get_overlap(s1, s2):\n",
    "    s = difflib.SequenceMatcher(None, s1, s2)\n",
    "    pos_a, pos_b, size = s.find_longest_match(0, len(s1), 0, len(s2)) \n",
    "    return pos_a, pos_b, size\n",
    "\n",
    "\n",
    "def match(edges, indices, query_str, tokenizer, n_paths=NPATHS):\n",
    "    query = tokenizer.tokenizer(query_str)['input_ids']\n",
    "    query = [query[i:i + 2] for i in range(0, len(query)-1, 1)]\n",
    "    \n",
    "    # filter bigrams\n",
    "    bigrams = []\n",
    "    for e in edges:\n",
    "        bigram = [indices[e[0]], indices[e[1]]]\n",
    "        if bigram in query:\n",
    "            bigrams.append(e)\n",
    "\n",
    "    # build graph\n",
    "    DG = nx.DiGraph()\n",
    "    DG.add_edges_from(bigrams)\n",
    "    \n",
    "    # find all paths\n",
    "#     all_paths = []\n",
    "#     for (x, y) in itertools.combinations(DG.nodes, 2):\n",
    "#         for path in nx.all_simple_paths(DG, x, y):\n",
    "#             all_paths.append(path)\n",
    "#     # sort all paths\n",
    "#     all_paths.sort(key=len, reverse=True)\n",
    "    \n",
    "    all_paths = [nx.dag_longest_path(DG)]\n",
    "\n",
    "    # lookup maximum overlap between strings\n",
    "    for path in all_paths[:n_paths]:\n",
    "        word = ''.join(tokenizer.tokenizer.convert_ids_to_tokens([indices[i] for i in path]))\n",
    "#         print(word)\n",
    "        pos_a, pos_b, size = get_overlap(query_str, word)\n",
    "        overlap = query_str[pos_a:pos_a+size]\n",
    "#         print(overlap)\n",
    "        overlap_indices = path[pos_a:pos_a+size]\n",
    "        return len(overlap) / len(query_str), sum(indices_logits[overlap_indices])\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiebke carolsfeld\n",
      "['wiebke', 'carolsfeld']\n",
      "1.00 words matched with mass 116.41\n"
     ]
    }
   ],
   "source": [
    "# encode entity label\n",
    "query_str = s_label\n",
    "# query_str = 'gregor'\n",
    "print(query_str)\n",
    "\n",
    "q_words = [w for w in query_str.split() if len(w) > 1]\n",
    "print(q_words)\n",
    "\n",
    "acc_matches, acc_logits = 0, 0\n",
    "for word in q_words:\n",
    "    query_str = word.upper()\n",
    "    matches, logits = match(edges, indices, query_str, tokenizer)\n",
    "    acc_matches += matches\n",
    "    acc_logits += logits\n",
    "    \n",
    "print('%.2f words matched with mass %.2f' % (acc_matches/len(q_words), acc_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wiebke', 'carolsfeld']\n",
      "1.00 words matched with mass 116.41\n",
      "['film', 'director']\n",
      "1.00 words matched with mass 77.69\n"
     ]
    }
   ],
   "source": [
    "# search through all pre-selected entity labels\n",
    "for query_str in top_labels:\n",
    "#     print(query_str)\n",
    "\n",
    "    q_words = [w for w in query_str.split() if len(w) > 1]\n",
    "    \n",
    "    acc_matches, acc_logits = 0, 0\n",
    "    for word in q_words:\n",
    "        query_str = word.upper()\n",
    "        matches, logits = match(edges, indices, query_str, tokenizer)\n",
    "        acc_matches += matches\n",
    "        acc_logits += logits\n",
    "    if acc_matches/len(q_words) == 1 and acc_logits > 70:\n",
    "        print(q_words)\n",
    "        print('%.2f words matched with mass %.2f' % (acc_matches/len(q_words), acc_logits))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
