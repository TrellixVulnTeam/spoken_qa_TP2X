{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function.Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([318, 3])\n",
      "what is a film directed by weepcavon carolls feld\n"
     ]
    }
   ],
   "source": [
    "SAMPLE = 2  # pick sample from the dev set\n",
    "\n",
    "# match entity label to the ctc table\n",
    "import os\n",
    "import sympy\n",
    "\n",
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "\n",
    "CTC_DEPTH = 3  # size of the ctc matrix considered for search\n",
    "NPATHS = 10 # number of longest paths on the bigram graph\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "tokenizer = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "\n",
    "# path = \"../data/dev/\"\n",
    "path = \"../data/gtts/dev/\"\n",
    "\n",
    "\n",
    "file = str(SAMPLE) + '.wav'\n",
    "speech, samplerate = sf.read(path+file)\n",
    "i = int(file.split('.')[0]) - 1\n",
    "\n",
    "input_values = tokenizer(speech, return_tensors=\"pt\", padding=\"longest\").input_values\n",
    "logits = model(input_values).logits\n",
    "\n",
    "# find where s_tokens appear in the table\n",
    "ctc_table = torch.topk(logits, k=CTC_DEPTH, dim=-1)\n",
    "predicted_ids = ctc_table.indices[0]\n",
    "# predicted_ids = torch.argmax(logits, dim=-1).indices\n",
    "\n",
    "# print(predicted_ids)\n",
    "print(predicted_ids.shape)\n",
    "\n",
    "# greedy decoding\n",
    "transcription = tokenizer.batch_decode(torch.argmax(logits, dim=-1))[0].lower()\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8708\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = np.transpose(np.array(predicted_ids))\n",
    "indices = predictions.flatten()\n",
    "# \n",
    "logits = ctc_table.values[0]\n",
    "predictions_logits = np.transpose(logits.detach().numpy())\n",
    "indices_logits = predictions_logits.flatten()\n",
    "\n",
    "# print(logits)\n",
    "# probs[logits < 0] = 0  # drop negative logits\n",
    "probs = torch.nn.functional.softmax(logits, dim = 1)\n",
    "# probs[probs < 2.e-07] = 0  # drop negative logits\n",
    "# print(probs)\n",
    "prediction_probs = np.transpose(probs.detach().numpy())\n",
    "indices_probs = prediction_probs.flatten()\n",
    "\n",
    "\n",
    "# generate adjacencies\n",
    "def connect(predictions, t, k, n):\n",
    "    edges = []\n",
    "    for j in range(predictions.shape[0]):  # offset\n",
    "        if predictions[j][k] != 0:\n",
    "            if predictions_logits[j][k] > 0:\n",
    "                edges.append([n*predictions.shape[1]+t, j*predictions.shape[1]+k, predictions_logits[j][k]])\n",
    "        else:\n",
    "            # skip to next if exists\n",
    "            if k < predictions.shape[1]-1:\n",
    "                edges.extend(connect(predictions, t, k+1, n))\n",
    "    return edges\n",
    "\n",
    "edges = []\n",
    "for t in range(predictions.shape[1]-1):  # columns\n",
    "    for n in range(predictions.shape[0]):  # rows\n",
    "        if predictions[n][t] != 0:\n",
    "            edges.extend(connect(predictions, t, t+1, n))\n",
    "\n",
    "print(len(edges))\n",
    "# 6107"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28497 entity labels\n"
     ]
    }
   ],
   "source": [
    "# load entities\n",
    "import json\n",
    "\n",
    "path = '../data/'\n",
    "\n",
    "with open(path+'entities.json', 'r') as fin:\n",
    "    entities = json.load(fin)\n",
    "print(len(entities), 'entity labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['head of government', 'head of government']\n"
     ]
    }
   ],
   "source": [
    "# load relations\n",
    "from predicates import properties\n",
    "\n",
    "relations = {}\n",
    "for p in properties['results']['bindings']:\n",
    "    label = p['propertyLabel']['value']\n",
    "\n",
    "    _id = p['property']['value'].split('/')[-1]\n",
    "    relations[_id] = label\n",
    "\n",
    "    _id = 'R' + p['property']['value'].split('/')[-1][1:]\n",
    "    relations[_id] = label\n",
    "\n",
    "# all unique predicate labels\n",
    "all_predicate_labels = list(relations.values())\n",
    "print(all_predicate_labels[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R57 Q14949730\n",
      "what is a film directed by wiebke von carolsfeld\n",
      "wiebke carolsfeld\n",
      "director\n"
     ]
    }
   ],
   "source": [
    "# load original question\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"]'\n",
    "\n",
    "path = '../data/'\n",
    "with open(path+'annotated_wd_data_valid_answerable.txt') as fin:\n",
    "    lines = fin.readlines()\n",
    "    l = lines[SAMPLE-1]\n",
    "#         subject [tab] property [tab] object [tab] question\n",
    "    s, p, o, q = l.strip('\\n').split('\\t')\n",
    "    print(p, o)\n",
    "    \n",
    "    q = re.sub(chars_to_ignore_regex, '', q).lower()\n",
    "    q = unidecode(q)\n",
    "    q = ''.join([j for i, j in enumerate(q) if j != q[i-1]])  # remove repeated letters\n",
    "    print(q)\n",
    "\n",
    "    s_label = entities[s]\n",
    "    s_label = re.sub(chars_to_ignore_regex, '', s_label).lower()\n",
    "    s_label = unidecode(s_label)\n",
    "    s_label = ''.join([j for i, j in enumerate(s_label) if j != s_label[i-1]])  # remove repeated letters\n",
    "    print(s_label)\n",
    "    \n",
    "    p_label = relations[p]\n",
    "    p_label = re.sub(chars_to_ignore_regex, '', p_label).lower()\n",
    "    p_label = unidecode(p_label)\n",
    "    p_label = ''.join([j for i, j in enumerate(p_label) if j != p_label[i-1]])  # remove repeated letters\n",
    "    print(p_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get all bigrams\n",
    "# bigrams = []\n",
    "# for e in edges:\n",
    "#     i1 = indices[e[0]]\n",
    "#     i2 = indices[e[1]]\n",
    "#     # skip repeated predictions of the same letters (not a bigram)\n",
    "#     if i1 != i2 and i1 not in special_tokens and i2 not in special_tokens:\n",
    "#         bigrams.append(''.join(vocabulary([i1, i2])).lower())  # normalise to lower case\n",
    "# # count all bigrams frequency in ctc matrix\n",
    "# bigrams = Counter(bigrams)\n",
    "# bottom_bigrams = [k.lower() for k, v in sorted(bigrams.items(), key=lambda x: x[1], reverse=True)][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 entity bigrams\n",
      "866 relation bigrams\n"
     ]
    }
   ],
   "source": [
    "# load indices\n",
    "import json\n",
    "\n",
    "path = '../data/'\n",
    "\n",
    "with open(path+'entities_bigrams_index.json', 'r') as fin:\n",
    "    entity_bigrams = json.load(fin)\n",
    "print(len(entity_bigrams), 'entity bigrams')\n",
    "\n",
    "with open(path+'relations_bigrams_index.json', 'r') as fin:\n",
    "    relation_bigrams = json.load(fin)\n",
    "print(len(relation_bigrams), 'relation bigrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is a film directed by wepcavon carols feld\n",
      "29\n",
      "['film', 'edison', 'charon']\n",
      "True\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "# check bottom up correct entity label is retrievable\n",
    "from collections import Counter\n",
    "\n",
    "# bottom up from greedy decoding\n",
    "t = ''.join([j for i, j in enumerate(transcription) if j != transcription[i-1]]).lower()  # remove repeated letters\n",
    "# t = \"vipcafon caronsfelt\"\n",
    "print(t)\n",
    "bottom_bigrams = set([word[i:i + 2] for word in t.split() for i in range(0, len(word)-1, 1)])\n",
    "print(len(bottom_bigrams))\n",
    "\n",
    "# match to the bigrams from the greedy decoded transcription\n",
    "e_labels = Counter()\n",
    "for b in bottom_bigrams:\n",
    "    matched_e_labels = entity_bigrams[b]\n",
    "    for e_label in matched_e_labels:\n",
    "        # how many bigrams did the label match normalised by the length of the label\n",
    "        e_labels[e_label] += 1#/len(e_label)\n",
    "# print(len(e_labels))\n",
    "\n",
    "# normalise by label length\n",
    "for l, c in e_labels.items():\n",
    "    e_labels[l] = c/len(l)\n",
    "    \n",
    "TOPN = 500\n",
    "top_e_labels = [l for l, c in e_labels.most_common(TOPN)]\n",
    "print(top_e_labels[:3])\n",
    "# print(len(top_e_labels))\n",
    "\n",
    "print(s_label in top_e_labels)\n",
    "print(top_e_labels.index(s_label))\n",
    "# print(e_labels[s_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['direction', 'director', 'directions']\n",
      "director\n",
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# check bottom up correct property label is retrievable\n",
    "p_labels = Counter()\n",
    "for b in bottom_bigrams:\n",
    "    matched_p_labels = relation_bigrams[b]\n",
    "    for label in matched_p_labels:\n",
    "        # how many bigrams did the label match normalised by the length of the label\n",
    "        p_labels[label] += 1#/len(e_label)\n",
    "# print(len(e_labels))\n",
    "\n",
    "# normalise by label length\n",
    "for l, c in p_labels.items():\n",
    "    p_labels[l] = c/len(l)\n",
    "    \n",
    "TOPN = 500\n",
    "top_p_labels = [l for l, c in p_labels.most_common(TOPN)]\n",
    "print(top_p_labels[:3])\n",
    "# print(len(top_e_labels))\n",
    "\n",
    "print(p_label)\n",
    "matched = p_label in top_p_labels\n",
    "print(matched)\n",
    "if matched:\n",
    "    print(top_p_labels.index(p_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load KG\n",
    "# from hdt import HDTDocument, TripleComponentRole\n",
    "\n",
    "# hdt_path = \"/ivi/ilps/personal/svakule/\"\n",
    "# hdt_file = 'wikidata2018_09_11.hdt'\n",
    "# # hdt_file = 'wikidata20200309.hdt'\n",
    "\n",
    "# PREFIX_E = 'http://www.wikidata.org/entity/'\n",
    "# PREFIX_P = 'http://www.wikidata.org/prop/direct/P'\n",
    "\n",
    "# kg = HDTDocument(hdt_path+hdt_file)\n",
    "\n",
    "# # Display some metadata about the HDT document itself\n",
    "# print(\"nb triples: %i\" % kg.total_triples)\n",
    "# print(\"nb subjects: %i\" % kg.nb_subjects)\n",
    "# print(\"nb predicates: %i\" % kg.nb_predicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check all possible triples using entity and relation candidates by retrieving the subgraph\n",
    "# path = '../data/'\n",
    "# max_triples = 50000\n",
    "# offset = 0\n",
    "\n",
    "# with open(path+'entities_labels2ids.json', 'r') as fin:\n",
    "#     entities = json.load(fin)\n",
    "# print(len(entities), 'entity labels in total')\n",
    "\n",
    "# with open(path+'relations_labels2ids.json', 'r') as fin:\n",
    "#     relations = json.load(fin)\n",
    "# print(len(relations), 'relation labels in total')\n",
    "\n",
    "# # look up entity ids in the KG\n",
    "# matched_entity_ids = []\n",
    "# print(len(top_e_labels))\n",
    "# for entity_label in top_e_labels:\n",
    "#     # look up entites by label\n",
    "#     for e_id in entities[entity_label]:\n",
    "#         entity = PREFIX_E + e_id\n",
    "#         matched_entity_ids.append(kg.string_to_global_id(entity, TripleComponentRole.OBJECT))\n",
    "# print(len(matched_entity_ids), 'entities matched')\n",
    "\n",
    "# # look up predicate ids in the KG\n",
    "# matched_relation_ids = []\n",
    "# print(len(top_p_labels))\n",
    "# for p_label in top_p_labels:\n",
    "#     # look up entites by label\n",
    "#     for p_id in relations[p_label]:\n",
    "#         predicate = PREFIX_P+p_id[1:]\n",
    "#         matched_relation_ids.append(kg.string_to_global_id(predicate, TripleComponentRole.PREDICATE))\n",
    "# print(len(matched_relation_ids), 'relations matched')\n",
    "\n",
    "# kg.configure_hops(1, matched_relation_ids, 'predef-wikidata2018-09-all', True, False)\n",
    "# subgraph = kg.compute_hops(matched_entity_ids, max_triples, offset)\n",
    "# entity_ids, predicate_ids, adjacencies = subgraph\n",
    "\n",
    "# n_entities = len(entity_ids)\n",
    "# n_relations = len(predicate_ids)\n",
    "# print(\"Subgraph with %d entities and %d relation types\"%(n_entities, n_relations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load adjacencies\n",
    "# def generate_adj_sp(adjacencies, n_entities, include_inverse):\n",
    "#     '''\n",
    "#     Build adjacency matrix\n",
    "#     '''\n",
    "#     adj_shape = (n_entities, n_entities)\n",
    "    \n",
    "#     # colect all predicate matrices separately into a list\n",
    "#     sp_adjacencies = []\n",
    "#     for edges in adjacencies:\n",
    "#         # split subject (row) and object (col) node URIs\n",
    "#         n_edges = len(edges)\n",
    "#         row, col = np.transpose(edges)\n",
    "        \n",
    "#         # duplicate edges in the opposite direction\n",
    "#         if include_inverse:\n",
    "#             _row = np.hstack([row, col])\n",
    "#             col = np.hstack([col, row])\n",
    "#             row = _row\n",
    "#             n_edges *= 2\n",
    "        \n",
    "#         # create adjacency matrix for this predicate\n",
    "#         data = np.ones(n_edges)\n",
    "#         adj = sp.csr_matrix((data, (row, col)), shape=adj_shape)\n",
    "#         sp_adjacencies.append(adj)\n",
    "    \n",
    "#     return np.asarray(sp_adjacencies)\n",
    "\n",
    "\n",
    "# # seed activation\n",
    "# x = np.zeros(n_entities)\n",
    "# for e in matched_entity_ids:\n",
    "#     idx = entity_ids.index(e)\n",
    "#     x[idx] = 1\n",
    "\n",
    "# A = generate_adj_sp(adjacencies, n_entities, include_inverse=True)\n",
    "# for i, _A in enumerate(A):\n",
    "#     # MP\n",
    "#     _y = x @ _A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378\n",
      "['HE', 'AE', 'LE', 'TE', 'RE', 'HA', 'TH', 'ED', 'ET', 'HT']\n"
     ]
    }
   ],
   "source": [
    "# ? represent speech as bigrams with probs\n",
    "from collections import defaultdict\n",
    "\n",
    "special_tokens = [4]\n",
    "vocabulary = tokenizer.tokenizer.convert_ids_to_tokens\n",
    "\n",
    "bigrams = defaultdict(int)\n",
    "for e in edges:\n",
    "    i1 = indices[e[0]]\n",
    "    i2 = indices[e[1]]\n",
    "    # skip repeated predictions of the same letters (not a bigram)\n",
    "    if i1 != i2 and i1 not in special_tokens and i2 not in special_tokens:\n",
    "        bigrams[''.join(vocabulary([i1, i2]))] += indices_logits[e[0]] + indices_logits[e[1]]\n",
    "print(len(bigrams))\n",
    "\n",
    "print([k for k, v in sorted(bigrams.items(), key=lambda x: x[1], reverse=True)][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bottom up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "# e_labels = Counter()\n",
    "# for b, c in bigrams.most_common(10):\n",
    "#     e_ids = entity_bigrams[b.lower()]\n",
    "#     for e_id in e_ids:\n",
    "#         e_label = list(entities.values())[e_id]\n",
    "#         e_labels[e_label] += 1\n",
    "# print(len(e_labels))\n",
    "\n",
    "# top_labels = [l for l, c in e_labels.items() if c > 1]\n",
    "# print(len(top_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiebke\n",
      "carolsfeld\n",
      "ca\n",
      "ar\n",
      "ro\n",
      "ol\n",
      "ls\n",
      "fe\n",
      "el\n",
      "ld\n"
     ]
    }
   ],
   "source": [
    "# check entity matches top bigrams\n",
    "# top_bigrams = [k.lower() for k, v in sorted(bigrams.items(), key=lambda x: x[1], reverse=True)][:20]\n",
    "# [b for b, c in bigrams.most_common(10)]\n",
    "# print(top_bigrams)\n",
    "\n",
    "for query in s_label.lower().split():\n",
    "    print(query)\n",
    "    for i in range(0, len(query)-1, 1):\n",
    "        bigram = query[i:i + 2]\n",
    "        if bigram in bottom_bigrams:\n",
    "            print(bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import itertools\n",
    "import difflib\n",
    "\n",
    "\n",
    "def get_overlap(s1, s2):\n",
    "    s = difflib.SequenceMatcher(None, s1, s2)\n",
    "    pos_a, pos_b, size = s.find_longest_match(0, len(s1), 0, len(s2)) \n",
    "    return pos_a, pos_b, size\n",
    "\n",
    "\n",
    "def match(edges, indices, query_str, tokenizer, n_paths=NPATHS):\n",
    "    query = tokenizer.tokenizer(query_str)['input_ids']\n",
    "    query = [query[i:i + 2] for i in range(0, len(query)-1, 1)]\n",
    "    \n",
    "    # filter bigrams\n",
    "    bigrams = []\n",
    "    for e in edges:\n",
    "        bigram = [indices[e[0]], indices[e[1]]]\n",
    "        if bigram in query:\n",
    "            bigrams.append(e)\n",
    "\n",
    "    # build graph\n",
    "    DG = nx.DiGraph()\n",
    "    DG.add_weighted_edges_from(bigrams)\n",
    "    \n",
    "    # find all paths\n",
    "#     all_paths = []\n",
    "#     for (x, y) in itertools.combinations(DG.nodes, 2):\n",
    "#         for path in nx.all_simple_paths(DG, x, y):\n",
    "#             all_paths.append(path)\n",
    "#     # sort all paths\n",
    "#     all_paths.sort(key=len, reverse=True)\n",
    "    \n",
    "    all_paths = [nx.dag_longest_path(DG)]\n",
    "\n",
    "    # lookup maximum overlap between strings\n",
    "    for path in all_paths[:n_paths]:\n",
    "        word = ''.join(tokenizer.tokenizer.convert_ids_to_tokens([indices[i] for i in path]))\n",
    "#         print(word)\n",
    "        pos_a, pos_b, size = get_overlap(query_str, word)\n",
    "        overlap = query_str[pos_a:pos_a+size]\n",
    "#         print(overlap)\n",
    "        overlap_indices = path[pos_a:pos_a+size]\n",
    "#         print(indices_probs[overlap_indices])\n",
    "#         print(np.prod(indices_probs[overlap_indices]))\n",
    "        return len(overlap) / len(query_str), np.sum(indices_logits[overlap_indices]) #sum(indices_logits[overlap_indices])\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiebke carolsfeld\n",
      "['wiebke', 'carolsfeld']\n",
      "0.92 words matched with accumulated logits over bigrams 147.73\n"
     ]
    }
   ],
   "source": [
    "# encode entity label\n",
    "query_str = s_label\n",
    "# query_str = 'carol red'\n",
    "print(query_str)\n",
    "\n",
    "q_words = [w for w in query_str.split() if len(w) > 1]\n",
    "print(q_words)\n",
    "\n",
    "acc_matches, acc_logits = 0, 0\n",
    "for word in q_words:\n",
    "    matches, p = match(edges, indices, word.upper(), tokenizer)\n",
    "    acc_matches += matches\n",
    "    acc_logits += p\n",
    "#     print(logits)\n",
    "    \n",
    "print('%.2f words matched with accumulated logits over bigrams %.2f' % (acc_matches/len(q_words), acc_logits))\n",
    "# print('matched with score %.2f' % (acc_matches/len(q_words) * acc_logits/len(q_words) * (e_labels[query_str]/len(query_str))))#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "film director\n",
      "['film', 'director']\n",
      "0.94 words matched with accumulated logits 117.82028198\n",
      "0.6153846153846154\n",
      "wiebke carolsfeld\n",
      "['wiebke', 'carolsfeld']\n",
      "0.92 words matched with accumulated logits 147.73165321\n",
      "0.47058823529411764\n",
      "harold harefot\n",
      "['harold', 'harefot']\n",
      "1.00 words matched with accumulated logits 100.10855865\n",
      "0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "# search through all pre-selected entity labels\n",
    "recognised_e_labels = []\n",
    "for query_str in top_e_labels:\n",
    "    \n",
    "    q_words = [w for w in query_str.split() if len(w) > 1]\n",
    "    \n",
    "    acc_matches, acc_logits = 0, 0\n",
    "    for word in q_words:\n",
    "        matches, logits = match(edges, indices, word.upper(), tokenizer)\n",
    "        acc_matches += matches\n",
    "        acc_logits += logits\n",
    "    if acc_matches/len(q_words) > 0.9 and acc_logits > 100:\n",
    "#     score = acc_matches/len(q_words) * acc_logits/len(q_words) * (e_labels[query_str]/len(query_str))\n",
    "#     if score > 25:\n",
    "        print(query_str)\n",
    "        recognised_e_labels.append(query_str)\n",
    "        print(q_words)\n",
    "        print('%.2f words matched with accumulated logits %.8f' % (acc_matches/len(q_words), acc_logits))#     break\n",
    "#         print('matched with score %.2f' % score)#     break\n",
    "        print(e_labels[query_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "director\n",
      "['director']\n",
      "0.88 words matched with accumulated logits over bigrams 8.10\n"
     ]
    }
   ],
   "source": [
    "# encode entity label\n",
    "query_str = p_label\n",
    "# query_str = 'carol red'\n",
    "print(query_str)\n",
    "\n",
    "q_words = [w for w in query_str.split() if len(w) > 1]\n",
    "print(q_words)\n",
    "\n",
    "acc_matches, acc_logits = 0, 0\n",
    "for word in q_words:\n",
    "    matches, p = match(edges, indices, word.upper(), tokenizer)\n",
    "    acc_matches += matches\n",
    "    acc_logits += p\n",
    "#     print(logits)\n",
    "    \n",
    "print('%.2f words matched with accumulated logits over bigrams %.2f' % (acc_matches/len(q_words), acc_logits/len(query_str)))\n",
    "# print('matched with score %.2f' % (acc_matches/len(q_words) * acc_logits/len(q_words) * (e_labels[query_str]/len(query_str))))#     break\n",
    "\n",
    "# what is a film directed by wepcavon carols feld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "director\n",
      "['director']\n",
      "0.88 words matched with accumulated logits 8.10374546\n",
      "0\n",
      "fe\n",
      "['fe']\n",
      "1.00 words matched with accumulated logits 12.69648266\n",
      "0\n",
      "edb film id\n",
      "['edb', 'film', 'id']\n",
      "0.89 words matched with accumulated logits 8.77005144\n",
      "0\n",
      "efis film id\n",
      "['efis', 'film', 'id']\n",
      "0.92 words matched with accumulated logits 8.37313096\n",
      "0\n",
      "sed\n",
      "['sed']\n",
      "1.00 words matched with accumulated logits 9.83647156\n",
      "0\n",
      "caries\n",
      "['caries']\n",
      "1.00 words matched with accumulated logits 8.87071737\n",
      "0\n",
      "par\n",
      "['par']\n",
      "1.00 words matched with accumulated logits 9.18140539\n",
      "0\n",
      "nf film id\n",
      "['nf', 'film', 'id']\n",
      "1.00 words matched with accumulated logits 8.71086884\n",
      "0\n",
      "elfilm film id\n",
      "['elfilm', 'film', 'id']\n",
      "0.94 words matched with accumulated logits 8.72986725\n",
      "0\n",
      "iafd film id\n",
      "['iafd', 'film', 'id']\n",
      "0.92 words matched with accumulated logits 8.10089032\n",
      "0\n",
      "sped\n",
      "['sped']\n",
      "1.00 words matched with accumulated logits 9.51506042\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# search through all pre-selected relation labels\n",
    "recognised_p_labels = []\n",
    "\n",
    "for query_str in top_p_labels:\n",
    "    \n",
    "    q_words = [w for w in query_str.split() if len(w) > 1]\n",
    "    \n",
    "    acc_matches, acc_logits = 0, 0\n",
    "    for word in q_words:\n",
    "        matches, logits = match(edges, indices, word.upper(), tokenizer)\n",
    "        acc_matches += matches\n",
    "        acc_logits += logits\n",
    "    if acc_matches/len(q_words) > 0.8 and acc_logits/len(query_str) > 8:\n",
    "#     score = acc_matches/len(q_words) * acc_logits/len(q_words) * (e_labels[query_str]/len(query_str))\n",
    "#     if score > 25:\n",
    "        print(query_str)\n",
    "        recognised_p_labels.append(query_str)\n",
    "        print(q_words)\n",
    "        print('%.2f words matched with accumulated logits %.8f' % (acc_matches/len(q_words), acc_logits/len(query_str)))#     break\n",
    "#         print('matched with score %.2f' % score)#     break\n",
    "        print(e_labels[query_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LTR features weight scores 1) retrieval 2) recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KGQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb triples: 2935160017\n",
      "nb subjects: 760717318\n",
      "nb predicates: 23387\n"
     ]
    }
   ],
   "source": [
    "# load KG\n",
    "from hdt import HDTDocument, TripleComponentRole\n",
    "\n",
    "hdt_path = \"/ivi/ilps/personal/svakule/\"\n",
    "hdt_file = 'wikidata2018_09_11.hdt'\n",
    "# hdt_file = 'wikidata20200309.hdt'\n",
    "\n",
    "PREFIX_E = 'http://www.wikidata.org/entity/'\n",
    "PREFIX_P = 'http://www.wikidata.org/prop/direct/P'\n",
    "\n",
    "kg = HDTDocument(hdt_path+hdt_file)\n",
    "\n",
    "# Display some metadata about the HDT document itself\n",
    "print(\"nb triples: %i\" % kg.total_triples)\n",
    "print(\"nb subjects: %i\" % kg.nb_subjects)\n",
    "print(\"nb predicates: %i\" % kg.nb_predicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27994 entity labels in total\n",
      "7335 relation labels in total\n",
      "500\n",
      "3 entities matched\n",
      "500\n",
      "22 relations matched\n",
      "Subgraph with 6 entities and 1 relation types\n"
     ]
    }
   ],
   "source": [
    "# find answers in KG\n",
    "# check all possible triples using entity and relation candidates by retrieving the subgraph\n",
    "path = '../data/'\n",
    "max_triples = 50000\n",
    "offset = 0\n",
    "\n",
    "with open(path+'entities_labels2ids.json', 'r') as fin:\n",
    "    entities = json.load(fin)\n",
    "print(len(entities), 'entity labels in total')\n",
    "\n",
    "with open(path+'relations_labels2ids.json', 'r') as fin:\n",
    "    relations = json.load(fin)\n",
    "print(len(relations), 'relation labels in total')\n",
    "\n",
    "# look up entity ids in the KG\n",
    "matched_entity_ids = []\n",
    "print(len(top_e_labels))\n",
    "for entity_label in recognised_e_labels:\n",
    "    # look up entites by label\n",
    "    for e_id in entities[entity_label]:\n",
    "        entity = PREFIX_E + e_id\n",
    "        matched_entity_ids.append(kg.string_to_global_id(entity, TripleComponentRole.OBJECT))\n",
    "print(len(matched_entity_ids), 'entities matched')\n",
    "\n",
    "# look up predicate ids in the KG\n",
    "matched_relation_ids = []\n",
    "print(len(top_p_labels))\n",
    "for p_label in recognised_p_labels:\n",
    "    # look up entites by label\n",
    "    for p_id in relations[p_label]:\n",
    "        predicate = PREFIX_P+p_id[1:]\n",
    "        matched_relation_ids.append(kg.string_to_global_id(predicate, TripleComponentRole.PREDICATE))\n",
    "print(len(matched_relation_ids), 'relations matched')\n",
    "\n",
    "namespace = 'predef-wikidata2018-09-all' # 'predef-wikidata2020-03-all'  # \n",
    "kg.configure_hops(1, matched_relation_ids, namespace, True, False)\n",
    "subgraph = kg.compute_hops(matched_entity_ids, max_triples, offset)\n",
    "entity_ids, predicate_ids, adjacencies = subgraph\n",
    "\n",
    "n_entities = len(entity_ids)\n",
    "n_relations = len(predicate_ids)\n",
    "print(\"Subgraph with %d entities and %d relation types\"%(n_entities, n_relations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['film director', 'wiebke carolsfeld', 'harold harefot']\n",
      "[15588676, 16030421, 8833144]\n",
      "[4956901, 16030421, 6161635, 15588676, 12869390, 23347336]\n",
      "[9780]\n",
      "['director', 'fe', 'edb film id', 'efis film id', 'sed', 'caries', 'par', 'nf film id', 'elfilm film id', 'iafd film id', 'sped']\n",
      "[9780, 9780, 6658, 6658, 7221, 7221, 0, 0, 0, 0, 6612, 6612, 7592, 7592, 5925, 5925, 7223, 7223, 9195, 9195, 6163, 6163]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'http://www.wikidata.org/prop/direct/P57'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(recognised_e_labels)\n",
    "print(matched_entity_ids)\n",
    "print(entity_ids)\n",
    "print(predicate_ids)\n",
    "print(recognised_p_labels)\n",
    "print(matched_relation_ids)\n",
    "kg.global_id_to_string(predicate_ids[0], TripleComponentRole.PREDICATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 0. 2. 0. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "import sympy\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "def generate_adj_sp(adjacencies, n_entities, include_inverse):\n",
    "    '''\n",
    "    Build adjacency matrix\n",
    "    '''\n",
    "    adj_shape = (n_entities, n_entities)\n",
    "    \n",
    "    # colect all predicate matrices separately into a list\n",
    "    sp_adjacencies = []\n",
    "    for edges in adjacencies:\n",
    "        # split subject (row) and object (col) node URIs\n",
    "        n_edges = len(edges)\n",
    "        row, col = np.transpose(edges)\n",
    "        \n",
    "        # duplicate edges in the opposite direction\n",
    "        if include_inverse:\n",
    "            _row = np.hstack([row, col])\n",
    "            col = np.hstack([col, row])\n",
    "            row = _row\n",
    "            n_edges *= 2\n",
    "        \n",
    "        # create adjacency matrix for this predicate\n",
    "        data = np.ones(n_edges)\n",
    "        adj = sp.csr_matrix((data, (row, col)), shape=adj_shape)\n",
    "        sp_adjacencies.append(adj)\n",
    "    \n",
    "    return np.asarray(sp_adjacencies)\n",
    "\n",
    "\n",
    "# seed activation with primes\n",
    "x = np.zeros(n_entities)\n",
    "weights = []\n",
    "for i, e in enumerate(matched_entity_ids):\n",
    "    if e in entity_ids:\n",
    "        idx = entity_ids.index(e)\n",
    "        prime = sympy.prime(i+1)\n",
    "        x[idx] = prime \n",
    "        weights.append(prime)\n",
    "\n",
    "# load adjacencies\n",
    "A = generate_adj_sp(adjacencies, n_entities, include_inverse=True)\n",
    "for i, _A in enumerate(A):\n",
    "    # MP\n",
    "    _y = x @ _A\n",
    "    print(_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15588676\n",
      "16030421\n"
     ]
    }
   ],
   "source": [
    "print(matched_entity_ids[weights.index(2)])\n",
    "print(matched_entity_ids[weights.index(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg.global_id_to_string(entity_ids[0], TripleComponentRole.OBJECT).split('/')[-1] == o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.wikidata.org/entity/Q16148301'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg.global_id_to_string(entity_ids[2], TripleComponentRole.OBJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
