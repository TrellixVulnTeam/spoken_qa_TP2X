{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify all paths\n",
    "dataset_name = 'WD18'\n",
    "data_path = '/ivi/ilps/personal/svakule/spoken_qa/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28497 entities\n",
      "{'_id': 'Q1938494', 'title': '', 'text': 'Miros≈Çaw Bork', 'metadata': {}}\n",
      "8913 relations\n",
      "{'_id': '9591', 'title': '', 'text': '#SOSBrutalism ID', 'metadata': {}}\n"
     ]
    }
   ],
   "source": [
    "# load entities and relations\n",
    "import os\n",
    "import json\n",
    "\n",
    "def store_beir_corpus(split):\n",
    "    path = '../data/%s.json' % split\n",
    "    corpus_path = data_path + dataset_name + \"/corpus\"\n",
    "    \n",
    "    with open(path, 'r') as fin:\n",
    "        items = json.load(fin)\n",
    "\n",
    "    corpus = []\n",
    "    for _id, label in items.items():\n",
    "        corpus.append({\"_id\": _id, \"title\": \"\", \"text\": label, \"metadata\": {}})\n",
    "\n",
    "    print(len(corpus), split)\n",
    "    print(corpus[0])\n",
    "    \n",
    "    if not os.path.exists(corpus_path):\n",
    "        os.makedirs(corpus_path)\n",
    "        \n",
    "    with open(corpus_path+\"/%s.jsonl\"%split, 'w') as out_file:\n",
    "        for d in corpus:\n",
    "            out_file.write(json.dumps(d))\n",
    "            out_file.write(\"\\n\")\n",
    "    \n",
    "    return items\n",
    "            \n",
    "entities = store_beir_corpus('entities')\n",
    "relations = store_beir_corpus('relations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Original Queries with Qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34374 train\n",
      "{'_id': 't0', 'text': 'what movie is produced by warner bros.', 'metadata': {}} ['t0', 'Q126399', '1'] ['t0', '272', '1']\n",
      "4867 valid\n",
      "{'_id': 'v0', 'text': 'Who was the trump ocean club international hotel and tower named after', 'metadata': {}} ['v1', 'Q318926', '1'] ['v0', '138', '1']\n",
      "39241 questions in total\n"
     ]
    }
   ],
   "source": [
    "# first load original dataset from https://github.com/askplatypus/wikidata-simplequestions\n",
    "\n",
    "def store_beir_qrels(split, qrels):\n",
    "    qrels_path = data_path + dataset_name + \"/qrels/%s.tsv\" % split\n",
    "    with open(qrels_path, 'w') as out_file:\n",
    "        for qrel in qrels:\n",
    "            out_file.write('\\t'.join(qrel)+'\\n')\n",
    "\n",
    "\n",
    "def process_wd_questions(split):\n",
    "    path_to_questions = data_path + 'annotated_wd_data_%s.txt' % split\n",
    "    \n",
    "    queries, rqrels, eqrels = [], [], []\n",
    "    with open(path_to_questions) as fin:\n",
    "        lines = fin.readlines()\n",
    "        for i, l in enumerate(lines):\n",
    "            s, p, o, q = lines[i].strip('\\n').split('\\t')\n",
    "            _id = split[0] + str(i)  # t0 for train split v0 for validation split\n",
    "            # store all queries\n",
    "            queries.append({\"_id\": _id, \"text\": q, \"metadata\": {}})\n",
    "            # filter questions with entities for which we have labels\n",
    "            if s in entities:\n",
    "                eqrels.append([_id, s, '1'])\n",
    "            # filter questions with relations for which we have labels\n",
    "            if p[1:] in relations:\n",
    "                rqrels.append([_id, p[1:], '1'])\n",
    "\n",
    "    # store entities and relations qrels separately for each split\n",
    "    store_beir_qrels(\"%s_entities\"%split, eqrels)\n",
    "    store_beir_qrels(\"%s_relations\"%split, rqrels)\n",
    "    \n",
    "    print(len(queries), split)\n",
    "    print(queries[0], eqrels[0], rqrels[0])\n",
    "    return queries\n",
    "\n",
    "\n",
    "queries = []\n",
    "queries = process_wd_questions(split='train')\n",
    "queries.extend(process_wd_questions(split='valid'))\n",
    "print(len(queries), 'questions in total')\n",
    "\n",
    "# save queries\n",
    "query_path = data_path + dataset_name + \"/queries/original.jsonl\"\n",
    "with open(query_path, 'w') as out_file:\n",
    "    for d in queries:\n",
    "        out_file.write(json.dumps(d))\n",
    "        out_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "\n",
    "# e.g. to test entity retrieval\n",
    "data_path = '/ivi/ilps/personal/svakule/spoken_qa/WD18/'\n",
    "corpus_path = data_path + 'corpus/entities.jsonl'  # relations.jsonl\n",
    "query_path = data_path + 'queries/original.jsonl'  # wav2vec2-base-960h.jsonl\n",
    "qrels_path = data_path + 'qrels/valid_entities.tsv'  # valid_relations.tsv train_entities.tsv train_relations.tsv\n",
    "\n",
    "corpus, queries, qrels = GenericDataLoader(corpus_file=corpus_path, \n",
    "                                           query_file=query_path, \n",
    "                                           qrels_file=qrels_path).load_custom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
