{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28497 entities\n",
      "{'_id': 'Q1938494', 'title': '', 'text': 'MirosÅ‚aw Bork', 'metadata': {}}\n",
      "8913 relations\n",
      "{'_id': '9591', 'title': '', 'text': '#SOSBrutalism ID', 'metadata': {}}\n"
     ]
    }
   ],
   "source": [
    "# load entities and relations\n",
    "import os\n",
    "import json\n",
    "\n",
    "from utils import data_path, dataset_name\n",
    "\n",
    "\n",
    "def store_beir_corpus(split):\n",
    "    path = '../data/%s.json' % split\n",
    "    corpus_path = data_path + dataset_name + \"/corpus\"\n",
    "    \n",
    "    with open(path, 'r') as fin:\n",
    "        items = json.load(fin)\n",
    "\n",
    "    corpus = []\n",
    "    for _id, label in items.items():\n",
    "        corpus.append({\"_id\": _id, \"title\": \"\", \"text\": label, \"metadata\": {}})\n",
    "\n",
    "    print(len(corpus), split)\n",
    "    print(corpus[0])\n",
    "    \n",
    "    if not os.path.exists(corpus_path):\n",
    "        os.makedirs(corpus_path)\n",
    "        \n",
    "    with open(corpus_path+\"/%s.jsonl\"%split, 'w') as out_file:\n",
    "        for d in corpus:\n",
    "            out_file.write(json.dumps(d))\n",
    "            out_file.write(\"\\n\")\n",
    "    \n",
    "    return items\n",
    "            \n",
    "entities = store_beir_corpus('entities')\n",
    "relations = store_beir_corpus('relations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ivi/ilps/personal/svakule/spoken_qa/datasets/WD18/corpus\n"
     ]
    }
   ],
   "source": [
    "# store an extended corpus for relations with alternative aliases and descriptions\n",
    "split = 'relations-extra'\n",
    "\n",
    "path = '../data/%s.json' % split\n",
    "corpus_path = os.path.join(data_path, dataset_name, \"corpus\")\n",
    "print(corpus_path)\n",
    "\n",
    "with open(path, 'r') as fin, open(corpus_path+\"/%s.jsonl\"%split, 'w') as out_file:\n",
    "    items = json.load(fin)\n",
    "    for e in items:\n",
    "        _id = e['id'][1:]\n",
    "\n",
    "        label = e['label']\n",
    "        aliases = ' '.join(e['aliases'])\n",
    "        description = str(e['description'])\n",
    "        text = ' '.join([label, aliases, description])\n",
    "\n",
    "        d = {\"_id\": _id, \"title\": label, \"text\": text, \"metadata\": {}}\n",
    "#         print(d)\n",
    "        out_file.write(json.dumps(d))\n",
    "        out_file.write(\"\\n\")\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Original Queries with Qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb triples: 2935160017\n",
      "nb subjects: 760717318\n",
      "nb predicates: 23387\n"
     ]
    }
   ],
   "source": [
    "# load KG to check triples are there and add alternative answers with the same s, p\n",
    "from kgqa import load_kg, check_triple\n",
    "\n",
    "kg = load_kg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4867 valid\n",
      "{'_id': 'v0', 'text': 'Who was the trump ocean club international hotel and tower named after', 'metadata': {}} ['v1', 'Q318926', '1'] ['v0', '138', '1'] ['v1', 'Q1010', '1']\n",
      "2 questions with more than one correct answer\n",
      "3534 questions without answer found in the KG\n",
      "34374 train\n",
      "{'_id': 't0', 'text': 'what movie is produced by warner bros.', 'metadata': {}} ['t0', 'Q126399', '1'] ['t0', '272', '1'] ['t4', 'Q2888523', '1']\n",
      "29 questions with more than one correct answer\n",
      "25082 questions without answer found in the KG\n",
      "39241 questions in total\n"
     ]
    }
   ],
   "source": [
    "# first load original dataset from https://github.com/askplatypus/wikidata-simplequestions\n",
    "\n",
    "def store_beir_qrels(split, qrels):\n",
    "    qrels_path = os.path.join(data_path, dataset_name, \"qrels/%s.tsv\"%split)\n",
    "    with open(qrels_path, 'w') as out_file:\n",
    "        for qrel in qrels:\n",
    "            out_file.write('\\t'.join(qrel)+'\\n')\n",
    "\n",
    "\n",
    "def process_wd_questions(split):\n",
    "    path_to_questions = '/ivi/ilps/personal/svakule/spoken_qa/annotated_wd_data_%s.txt' % split\n",
    "    \n",
    "    queries, rqrels, eqrels, aqrels, aqrels_all = [], [], [], [], []\n",
    "    answers_extended, answers_removed = 0, 0\n",
    "    with open(path_to_questions) as fin:\n",
    "        lines = fin.readlines()\n",
    "        for i, l in enumerate(lines):\n",
    "            s, p, o, q = lines[i].strip('\\n').split('\\t')\n",
    "            _id = split[0] + str(i)  # t0 for train split v0 for validation split\n",
    "            # store all queries\n",
    "            queries.append({\"_id\": _id, \"text\": q, \"metadata\": {}})\n",
    "            # filter questions with entities for which we have labels\n",
    "            if s in entities:\n",
    "                eqrels.append([_id, s, '1'])\n",
    "            # filter questions with relations for which we have labels\n",
    "            if p[1:] in relations:\n",
    "                rqrels.append([_id, p[1:], '1'])\n",
    "            \n",
    "            # extend the ground-truth answers by getting the set of all answers from KG using the s, p pattern\n",
    "            e_set = check_triple(kg, (s, p[1:], o))\n",
    "            if not e_set:\n",
    "                answers_removed += 1\n",
    "                # filter questions with entities for which we have labels\n",
    "                for e_id in e_set:\n",
    "                    if e_id in entities:  # add only the entities that are in our entity corpus\n",
    "                        aqrels_all.append([_id, e_id, '1'])  # add only to aqrels_all\n",
    "            if len(e_set) > 1:\n",
    "                answers_extended += 1\n",
    "        \n",
    "            # filter questions with entities for which we have labels\n",
    "            for e_id in e_set:\n",
    "                if e_id in entities:  # add only the entities that are in our entity corpus\n",
    "                    aqrels.append([_id, e_id, '1'])  # those qrels are filtered by the triples we found in this KG\n",
    "                    aqrels_all.append([_id, e_id, '1'])\n",
    "\n",
    "    # store entities and relations qrels separately for each split\n",
    "    store_beir_qrels(\"%s_entities\"%split, eqrels)\n",
    "    store_beir_qrels(\"%s_relations\"%split, rqrels)\n",
    "    store_beir_qrels(\"%s_answers\"%split, aqrels)\n",
    "    store_beir_qrels(\"%s_answers-all\"%split, aqrels_all)\n",
    "    \n",
    "    print(len(queries), split)\n",
    "    print(queries[0], eqrels[0], rqrels[0], aqrels[0])\n",
    "    print(\"%d questions with more than one correct answer\" % answers_extended)\n",
    "    print(\"%d questions without answer found in the KG\" % answers_removed)\n",
    "    return queries\n",
    "\n",
    "\n",
    "queries = []\n",
    "queries = process_wd_questions(split='valid')\n",
    "queries.extend(process_wd_questions(split='train'))\n",
    "print(len(queries), 'questions in total')\n",
    "\n",
    "# save queries\n",
    "query_path = os.path.join(data_path, dataset_name, \"queries/original.jsonl\")\n",
    "with open(query_path, 'w') as out_file:\n",
    "    for d in queries:\n",
    "        out_file.write(json.dumps(d))\n",
    "        out_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "\n",
    "# e.g. to test entity retrieval\n",
    "data_path = '/ivi/ilps/personal/svakule/spoken_qa/WD18/'\n",
    "corpus_path = data_path + 'corpus/entities.jsonl'  # relations.jsonl\n",
    "query_path = data_path + 'queries/original.jsonl'  # wav2vec2-base-960h.jsonl\n",
    "qrels_path = data_path + 'qrels/valid_entities.tsv'  # valid_relations.tsv train_entities.tsv train_relations.tsv\n",
    "\n",
    "corpus, queries, qrels = GenericDataLoader(corpus_file=corpus_path, \n",
    "                                           query_file=query_path, \n",
    "                                           qrels_file=qrels_path).load_custom()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Transcripts with ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "\n",
    "model_name = \"wav2vec2-large-960h-lv60-self\"  # wav2vec2-base-960h\"\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/\"+model_name)\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/\" + model_name)\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate transcripts\n",
    "import json\n",
    "from utils import dataset_name, data_path\n",
    "\n",
    "def generate_transcripts(split, query_path):\n",
    "    wav_path = \"/ivi/ilps/personal/svakule/spoken_qa/gtts/annotated_wd_data_%s/wav16000/\" % split\n",
    "\n",
    "    with open(query_path, 'a') as out_file:\n",
    "        for file in os.listdir(wav_path):\n",
    "            _id = file.split('.')[0] # t0 for train split v0 for validation split\n",
    "\n",
    "            speech, samplerate = sf.read(wav_path+file)\n",
    "            \n",
    "            input_values = processor(speech, return_tensors=\"pt\", padding=\"longest\",\n",
    "                                     sampling_rate=samplerate).input_values\n",
    "\n",
    "            input_values = input_values.to('cuda')\n",
    "\n",
    "            logits = model(input_values).logits\n",
    "            predicted_ids = torch.argmax(logits, dim=-1)\n",
    "            transcription = tokenizer.batch_decode(predicted_ids)[0].lower()\n",
    "            \n",
    "            # save\n",
    "            q = {\"_id\": _id, \"text\": transcription, \"metadata\": {}}\n",
    "            out_file.write(json.dumps(q)+\"\\n\")\n",
    "\n",
    "query_path = os.path.join(data_path, dataset_name, \"queries\", \"%s.jsonl\"%model_name)\n",
    "print(query_path)\n",
    "\n",
    "# generate_transcripts('valid', query_path)\n",
    "# generate_transcripts('train', query_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
