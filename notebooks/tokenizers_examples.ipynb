{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who is a musician born in detroit\n",
      "who is a musician born indetroit\n",
      "who is a musician born in las vegas\n"
     ]
    }
   ],
   "source": [
    "# load sample original and transcribed questions to compare\n",
    "from utils import load_queries\n",
    "\n",
    "q_id = 't1'\n",
    "\n",
    "queries_o = load_queries(queries_version='original')\n",
    "queries_t = load_queries(queries_version='wav2vec2-base-960h')\n",
    "print(queries_o[q_id])  # anchor\n",
    "print(queries_t[q_id])  # positive\n",
    "\n",
    "distractor_q = 'who is a musician born in las vegas'\n",
    "print(distractor_q)  # negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522 tokens\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained model\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# pre_trained_model_name = \"distilbert-base-uncased\"\n",
    "pre_trained_model_name = \"sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pre_trained_model_name) \n",
    "bert_model = AutoModel.from_pretrained(pre_trained_model_name)\n",
    "\n",
    "# check vocabulary size\n",
    "print(tokenizer.vocab_size, 'tokens')\n",
    "# show vocabulary\n",
    "v = tokenizer.get_vocab()\n",
    "# print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who is a musician born in detroit\n",
      "['[CLS]', 'who', 'is', 'a', 'musician', 'born', 'in', 'detroit', '[SEP]']\n",
      "who is a musician born indetroit\n",
      "['[CLS]', 'who', 'is', 'a', 'musician', 'born', 'ind', '##et', '##roi', '##t', '[SEP]']\n",
      "who is a musician born in las vegas\n",
      "['[CLS]', 'who', 'is', 'a', 'musician', 'born', 'in', 'las', 'vegas', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "print(queries_o[q_id])\n",
    "query_input_qo = tokenizer(queries_o[q_id], return_tensors=\"pt\")\n",
    "print(tokenizer.convert_ids_to_tokens(query_input_qo[\"input_ids\"][0]))\n",
    "\n",
    "print(queries_t[q_id])\n",
    "query_input_qt = tokenizer(queries_t[q_id], return_tensors=\"pt\")\n",
    "print(tokenizer.convert_ids_to_tokens(query_input_qt[\"input_ids\"][0]))\n",
    "\n",
    "print(distractor_q)\n",
    "query_input_qd = tokenizer(distractor_q, return_tensors=\"pt\")\n",
    "print(tokenizer.convert_ids_to_tokens(query_input_qd[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original score:  126.30570220947266\n",
      "Transcript score:  98.63985443115234\n",
      "Distractor score:  106.63993072509766\n"
     ]
    }
   ],
   "source": [
    "query_encoded_qo = bert_model(**query_input_qo)[0][:,0,:].squeeze(0)\n",
    "query_encoded_qt = bert_model(**query_input_qt)[0][:,0,:].squeeze(0)\n",
    "query_encoded_qd = bert_model(**query_input_qd)[0][:,0,:].squeeze(0)\n",
    "\n",
    "score0 = query_encoded_qo.dot(query_encoded_qo)\n",
    "print(\"Original score: \", float(score0))\n",
    "\n",
    "score = query_encoded_qo.dot(query_encoded_qt)\n",
    "print(\"Transcript score: \", float(score))\n",
    "\n",
    "score = query_encoded_qo.dot(query_encoded_qd)\n",
    "print(\"Distractor score: \", float(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 tokens\n"
     ]
    }
   ],
   "source": [
    "# load ASR model\n",
    "import os\n",
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "# model.to('cuda')\n",
    "\n",
    "# check vocabulary size\n",
    "print(processor.tokenizer.vocab_size, 'tokens')\n",
    "# show characters vocabulary\n",
    "v = processor.tokenizer.get_vocab()\n",
    "# print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who is a musician born in detroit\n",
      "['W', 'H', 'O', '|', 'I', 'S', '|', 'A', '|', 'M', 'U', 'S', 'I', 'C', 'I', 'A', 'N', '|', 'B', 'O', 'R', 'N', '|', 'I', 'N', '|', 'D', 'E', 'T', 'R', 'O', 'I', 'T']\n",
      "who is a musician born indetroit\n",
      "['W', 'H', 'O', '|', 'I', 'S', '|', 'A', '|', 'M', 'U', 'S', 'I', 'C', 'I', 'A', 'N', '|', 'B', 'O', 'R', 'N', '|', 'I', 'N', 'D', 'E', 'T', 'R', 'O', 'I', 'T']\n",
      "who is a musician born in las vegas\n",
      "['W', 'H', 'O', '|', 'I', 'S', '|', 'A', '|', 'M', 'U', 'S', 'I', 'C', 'I', 'A', 'N', '|', 'B', 'O', 'R', 'N', '|', 'I', 'N', '|', 'L', 'A', 'S', '|', 'V', 'E', 'G', 'A', 'S']\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "print(queries_o[q_id])\n",
    "query_input_qo = processor.tokenizer(queries_o[q_id].upper(), return_tensors=\"pt\")\n",
    "print(processor.tokenizer.convert_ids_to_tokens(query_input_qo[\"input_ids\"][0]))\n",
    "\n",
    "print(queries_t[q_id])\n",
    "query_input_qt = processor.tokenizer(queries_t[q_id].upper(), return_tensors=\"pt\")\n",
    "print(processor.tokenizer.convert_ids_to_tokens(query_input_qt[\"input_ids\"][0]))\n",
    "\n",
    "# distractor_q = 'who is a musician born in las vegas'\n",
    "print(distractor_q)\n",
    "query_input_qd = processor.tokenizer(distractor_q.upper(), return_tensors=\"pt\")\n",
    "print(processor.tokenizer.convert_ids_to_tokens(query_input_qd[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function.Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 224, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 16.0426, -26.5667, -26.2056,  ...,  -6.9358,  -6.9005,  -8.1346],\n",
       "         [ 16.0563, -26.6414, -26.2775,  ...,  -6.9886,  -6.9074,  -8.1332],\n",
       "         [ 15.9869, -26.5113, -26.1531,  ...,  -6.8883,  -6.8722,  -8.0371],\n",
       "         ...,\n",
       "         [ 15.8354, -26.4650, -26.1130,  ...,  -6.6700,  -6.8191,  -7.8721],\n",
       "         [ 15.8360, -26.6142, -26.2627,  ...,  -6.7499,  -6.9985,  -7.9503],\n",
       "         [ 15.7416, -27.0900, -26.7318,  ...,  -7.1503,  -7.4961,  -8.5756]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = 'train'\n",
    "q_id = 't1'\n",
    "\n",
    "wav_path = \"/ivi/ilps/personal/svakule/spoken_qa/gtts/annotated_wd_data_%s/wav/\" % split\n",
    "file = q_id + '.wav'\n",
    "\n",
    "speech, samplerate = sf.read(wav_path+file)\n",
    "input_values = processor(speech, return_tensors=\"pt\", padding=\"longest\").input_values\n",
    "# input_values = input_values.to('cuda')\n",
    "logits = model(input_values).logits\n",
    "print(logits.shape)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[18, 11,  8,  4, 10, 12,  4,  7,  4, 17, 16, 12, 10, 19, 10,  7,  9,  4,\n",
      "         24,  8, 13,  9,  4, 10,  9,  4, 15,  7, 12,  4, 25,  5, 21,  7, 12]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[15,  7, 12,  4, 25,  5, 21,  7, 12]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[24,  8, 13,  9,  4, 10,  9]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "q = 'who is a musician born in las vegas'\n",
    "e_label = 'las vegas'\n",
    "r_label = 'born in'\n",
    "\n",
    "query_input_q = processor.tokenizer(q.upper(), return_tensors=\"pt\")\n",
    "query_input_e = processor.tokenizer(e_label.upper(), return_tensors=\"pt\")\n",
    "query_input_r = processor.tokenizer(r_label.upper(), return_tensors=\"pt\")\n",
    "\n",
    "print(query_input_q)\n",
    "print(query_input_e)\n",
    "print(query_input_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DistilBertModel.get_input_embeddings of DistilBertModel(\n",
      "  (embeddings): Embeddings(\n",
      "    (word_embeddings): Embedding(30525, 768)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (layer): ModuleList(\n",
      "      (0): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained model\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# pre_trained_model_name = \"distilbert-base-uncased\"\n",
    "pre_trained_model_name = \"sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pre_trained_model_name) \n",
    "bert_model = AutoModel.from_pretrained(pre_trained_model_name)\n",
    "\n",
    "# add special tokens to tokenizer vocabulary and the model\n",
    "special_tokens_dict = {'additional_special_tokens': ['[Q]','[E]','[R]']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "bert_model.resize_token_embeddings(len(tokenizer))\n",
    "print(bert_model.get_input_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q]who is a musician born in las vegas\n",
      "['[CLS]', '[Q]', 'who', 'is', 'a', 'musician', 'born', 'in', 'las', 'vegas', '[SEP]']\n",
      "['[CLS]', '[E]', 'las', 'vegas', '[SEP]']\n",
      "['[CLS]', '[R]', 'born', 'in', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "q = '[Q]who is a musician born in las vegas'\n",
    "e_label = '[E]las vegas'\n",
    "r_label = '[R]born in'\n",
    "print(q)\n",
    "\n",
    "query_input_q = tokenizer(q, return_tensors=\"pt\")\n",
    "query_input_e = tokenizer(e_label, return_tensors=\"pt\")\n",
    "query_input_r = tokenizer(r_label, return_tensors=\"pt\")\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(query_input_q[\"input_ids\"][0]))\n",
    "print(tokenizer.convert_ids_to_tokens(query_input_e[\"input_ids\"][0]))\n",
    "print(tokenizer.convert_ids_to_tokens(query_input_r[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[121, 106, 113,  34, 107, 117,  34,  99,  34, 111, 119, 117, 107, 101,\n",
      "         107,  99, 112,  34, 100, 113, 116, 112,  34, 107, 112,  34, 110,  99,\n",
      "         117,  34, 120, 103, 105,  99, 117],\n",
      "        [110,  99, 117,  34, 120, 103, 105,  99, 117,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0],\n",
      "        [100, 113, 116, 112,  34, 107, 112,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0]])\n"
     ]
    }
   ],
   "source": [
    "# script from https://huggingface.co/google/reformer-enwik8\n",
    "import torch\n",
    "\n",
    "q = 'who is a musician born in las vegas'\n",
    "e_label = 'las vegas'\n",
    "r_label = 'born in'\n",
    "\n",
    "# Encoding\n",
    "def encode(list_of_strings, pad_token_id=0):\n",
    "    max_length = max([len(string) for string in list_of_strings])\n",
    "\n",
    "    # create emtpy tensors\n",
    "    attention_masks = torch.zeros((len(list_of_strings), max_length), dtype=torch.long)\n",
    "    input_ids = torch.full((len(list_of_strings), max_length), pad_token_id, dtype=torch.long)\n",
    "\n",
    "    for idx, string in enumerate(list_of_strings):\n",
    "        # make sure string is in byte format\n",
    "        if not isinstance(string, bytes):\n",
    "            string = str.encode(string)\n",
    "\n",
    "        input_ids[idx, :len(string)] = torch.tensor([x + 2 for x in string])\n",
    "        attention_masks[idx, :len(string)] = 1\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "encoded, attention_masks = encode([q, e_label, r_label])\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode\n",
    "import torch.nn.functional as F\n",
    "\n",
    "F.one_hot(encoded, num_classes=6)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
