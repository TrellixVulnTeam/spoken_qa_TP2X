{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load file a process to get the ctc matrix\n",
    "SAMPLE = 2  # pick sample from the dev set\n",
    "\n",
    "# match entity label to the ctc table\n",
    "import os\n",
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "\n",
    "CTC_DEPTH = 3  # size of the ctc matrix considered for search\n",
    "NPATHS = 10 # number of longest paths on the bigram graph\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "tokenizer = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "\n",
    "# path = \"../data/dev/\"\n",
    "path = \"../data/gtts/dev/\"\n",
    "\n",
    "\n",
    "file = str(SAMPLE) + '.wav'\n",
    "speech, samplerate = sf.read(path+file)\n",
    "i = int(file.split('.')[0]) - 1\n",
    "\n",
    "input_values = tokenizer(speech, return_tensors=\"pt\", padding=\"longest\").input_values\n",
    "logits = model(input_values).logits\n",
    "\n",
    "# find where s_tokens appear in the table\n",
    "ctc_table = torch.topk(logits, k=CTC_DEPTH, dim=-1)\n",
    "predicted_ids = ctc_table.indices[0]\n",
    "# predicted_ids = torch.argmax(logits, dim=-1).indices\n",
    "\n",
    "# print(predicted_ids)\n",
    "print(predicted_ids.shape)\n",
    "\n",
    "# greedy decoding\n",
    "transcription = tokenizer.batch_decode(torch.argmax(logits, dim=-1))[0].lower()\n",
    "print(transcription)\n",
    "\n",
    "# e: wiebke carolsfeld\n",
    "# r: director"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lexicon-constratined beam decode\n",
    "import numpy as np\n",
    "\n",
    "q = \"what is a film directed by wiebke von carolsfeld\"\n",
    "lexicon = q.lower().split()\n",
    "print(lexicon)\n",
    "\n",
    "predictions = np.transpose(np.array(predicted_ids))\n",
    "\n",
    "probs = torch.nn.functional.softmax(logits, dim = 1)\n",
    "prediction_probs = np.transpose(probs.detach().numpy())\n",
    "\n",
    "# find the position at which the word not in lexicon starts\n",
    "w = ''\n",
    "words = []\n",
    "w_start_position = 0\n",
    "for i, c_id in enumerate(predictions[0]):\n",
    "    if c_id != 0:\n",
    "        c = tokenizer.decode([c_id]).lower()\n",
    "        if not w:\n",
    "            w += c\n",
    "        elif c != w[-1]:\n",
    "            w += c\n",
    "        if c_id == 4 and w:\n",
    "            # check with lexicon\n",
    "            print(w)\n",
    "            if w in lexicon:\n",
    "                words.append(w)\n",
    "            else:\n",
    "#                 print(w_start_position)\n",
    "               \n",
    "                break\n",
    "#                 mismatch_position = i - len(w)\n",
    "#                 print(mismatch_position)\n",
    "#                 mismatch_character = tokenizer.decode([predictions[1][w_start_position]]).lower()\n",
    "#                 print(mismatch_character)\n",
    "            w = ''\n",
    "            w_start_position = i\n",
    "print(words)\n",
    "\n",
    "# cross decode by picking all possible sequences from each string\n",
    "v1 = predictions[0][w_start_position:]\n",
    "v2 = predictions[1][w_start_position:]\n",
    "v3 = predictions[2][w_start_position:]\n",
    "print(v1)\n",
    "print(v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all possible first letters\n",
    "offset0 = 0\n",
    "candidates = []\n",
    "for i, c_id in enumerate(predictions[0][w_start_position:][offset0:]):\n",
    "    if c_id not in [0,4]:\n",
    "        c = tokenizer.decode([c_id]).lower()\n",
    "        if c not in candidates:\n",
    "            for w in lexicon:\n",
    "                if w[:1] == c:\n",
    "                    candidates.append(c)\n",
    "                    break\n",
    "        break\n",
    "offset = i + 1\n",
    "\n",
    "# alternative first letters\n",
    "for i in range(CTC_DEPTH-1):\n",
    "    for i, c_id in enumerate(predictions[i+1][w_start_position:][offset0:offset]):\n",
    "        if c_id not in [0,4]:\n",
    "            c = tokenizer.decode([c_id]).lower()\n",
    "            if c not in candidates:\n",
    "                for w in lexicon:\n",
    "                    if w[:1] == c:\n",
    "                        candidates.append(c)\n",
    "                        break\n",
    "print(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all possible second letters\n",
    "\n",
    "def expand_sequences(predictions, candidates, offset0):\n",
    "    new_candidates = []\n",
    "    for i, c_id in enumerate(predictions[0][w_start_position:][offset0:]):\n",
    "        if c_id not in [0,4]:\n",
    "            c = tokenizer.decode([c_id]).lower()\n",
    "            for prefix in candidates:\n",
    "                prefix = prefix+c\n",
    "                if prefix not in new_candidates:\n",
    "                    for w in lexicon:\n",
    "    #                     print(w[:len(prefix)], prefix)\n",
    "                        if w[:len(prefix)] == prefix:\n",
    "                            new_candidates.append(prefix)\n",
    "                            break\n",
    "            break\n",
    "    offset = offset0 + i + 1\n",
    "\n",
    "    # alternative first letters\n",
    "    for i in range(CTC_DEPTH-1):\n",
    "        for i, c_id in enumerate(predictions[i+1][w_start_position:][offset0:offset]):\n",
    "            if c_id not in [0,4]:\n",
    "                c = tokenizer.decode([c_id]).lower()\n",
    "                for prefix in candidates:\n",
    "                    prefix = prefix+c\n",
    "                    if prefix not in new_candidates:\n",
    "                        for w in lexicon:\n",
    "                            if w[:len(prefix)] == prefix:\n",
    "                                new_candidates.append(prefix)\n",
    "                                break\n",
    "    return new_candidates, offset\n",
    "\n",
    "\n",
    "while candidates:\n",
    "    print(candidates)\n",
    "    candidates, offset = expand_sequences(predictions, candidates, offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
