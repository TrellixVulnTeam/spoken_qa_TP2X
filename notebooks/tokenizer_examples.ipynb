{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who is a musician born in detroit\n",
      "who is a musician born indetroit\n"
     ]
    }
   ],
   "source": [
    "# load sample original and transcribed questions to compare\n",
    "from utils import load_queries\n",
    "\n",
    "split = 'train'\n",
    "q_id = 't1'\n",
    "\n",
    "queries_o = load_queries(split='train', queries_version='original')\n",
    "queries_t = load_queries(split='train', queries_version='wav2vec2-base-960h')\n",
    "print(queries_o[q_id])\n",
    "print(queries_t[q_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# pre_trained_model_name = \"distilbert-base-uncased\"\n",
    "pre_trained_model_name = \"sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pre_trained_model_name) \n",
    "bert_model = AutoModel.from_pretrained(pre_trained_model_name)\n",
    "\n",
    "# check vocabulary size\n",
    "print(tokenizer.vocab_size, 'tokens')\n",
    "# show vocabulary\n",
    "v = tokenizer.get_vocab()\n",
    "# print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who is a musician born in detroit\n",
      "['[CLS]', 'who', 'is', 'a', 'musician', 'born', 'in', 'detroit', '[SEP]']\n",
      "who is a musician born indetroit\n",
      "['[CLS]', 'who', 'is', 'a', 'musician', 'born', 'ind', '##et', '##roi', '##t', '[SEP]']\n",
      "who is a musician born in las vegas\n",
      "['[CLS]', 'who', 'is', 'a', 'musician', 'born', 'in', 'las', 'vegas', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "print(queries_o[q_id])\n",
    "query_input_qo = tokenizer(queries_o[q_id], return_tensors=\"pt\")\n",
    "print(tokenizer.convert_ids_to_tokens(query_input_qo[\"input_ids\"][0]))\n",
    "\n",
    "print(queries_t[q_id])\n",
    "query_input_qt = tokenizer(queries_t[q_id], return_tensors=\"pt\")\n",
    "print(tokenizer.convert_ids_to_tokens(query_input_qt[\"input_ids\"][0]))\n",
    "\n",
    "distractor_q = 'who is a musician born in las vegas'\n",
    "print(distractor_q)\n",
    "query_input_qd = tokenizer(distractor_q, return_tensors=\"pt\")\n",
    "print(tokenizer.convert_ids_to_tokens(query_input_qd[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original score:  126.30570220947266\n",
      "Transcript score:  98.63985443115234\n",
      "Distractor score:  106.63993072509766\n"
     ]
    }
   ],
   "source": [
    "query_encoded_qo = bert_model(**query_input_qo)[0][:,0,:].squeeze(0)\n",
    "query_encoded_qt = bert_model(**query_input_qt)[0][:,0,:].squeeze(0)\n",
    "query_encoded_qd = bert_model(**query_input_qd)[0][:,0,:].squeeze(0)\n",
    "\n",
    "score0 = query_encoded_qo.dot(query_encoded_qo)\n",
    "print(\"Original score: \", float(score0))\n",
    "\n",
    "score = query_encoded_qo.dot(query_encoded_qt)\n",
    "print(\"Transcript score: \", float(score))\n",
    "\n",
    "score = query_encoded_qo.dot(query_encoded_qd)\n",
    "print(\"Distractor score: \", float(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 tokens\n"
     ]
    }
   ],
   "source": [
    "# load ASR model\n",
    "import os\n",
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model.to('cuda')\n",
    "\n",
    "# check vocabulary size\n",
    "print(processor.tokenizer.vocab_size, 'tokens')\n",
    "# show characters vocabulary\n",
    "v = processor.tokenizer.get_vocab()\n",
    "# print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who is a musician born in detroit\n",
      "['W', 'H', 'O', '|', 'I', 'S', '|', 'A', '|', 'M', 'U', 'S', 'I', 'C', 'I', 'A', 'N', '|', 'B', 'O', 'R', 'N', '|', 'I', 'N', '|', 'D', 'E', 'T', 'R', 'O', 'I', 'T']\n",
      "who is a musician born indetroit\n",
      "['W', 'H', 'O', '|', 'I', 'S', '|', 'A', '|', 'M', 'U', 'S', 'I', 'C', 'I', 'A', 'N', '|', 'B', 'O', 'R', 'N', '|', 'I', 'N', 'D', 'E', 'T', 'R', 'O', 'I', 'T']\n",
      "who is a musician born in las vegas\n",
      "['W', 'H', 'O', '|', 'I', 'S', '|', 'A', '|', 'M', 'U', 'S', 'I', 'C', 'I', 'A', 'N', '|', 'B', 'O', 'R', 'N', '|', 'I', 'N', '|', 'L', 'A', 'S', '|', 'V', 'E', 'G', 'A', 'S']\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "print(queries_o[q_id])\n",
    "query_input_qo = processor.tokenizer(queries_o[q_id].upper(), return_tensors=\"pt\")\n",
    "print(processor.tokenizer.convert_ids_to_tokens(query_input_qo[\"input_ids\"][0]))\n",
    "\n",
    "print(queries_t[q_id])\n",
    "query_input_qt = processor.tokenizer(queries_t[q_id].upper(), return_tensors=\"pt\")\n",
    "print(processor.tokenizer.convert_ids_to_tokens(query_input_qt[\"input_ids\"][0]))\n",
    "\n",
    "# distractor_q = 'who is a musician born in las vegas'\n",
    "print(distractor_q)\n",
    "query_input_qd = processor.tokenizer(distractor_q.upper(), return_tensors=\"pt\")\n",
    "print(processor.tokenizer.convert_ids_to_tokens(query_input_qd[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function.Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 224, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 16.0428, -26.5670, -26.2058,  ...,  -6.9360,  -6.9008,  -8.1345],\n",
       "         [ 16.0565, -26.6411, -26.2772,  ...,  -6.9885,  -6.9073,  -8.1329],\n",
       "         [ 15.9872, -26.5123, -26.1541,  ...,  -6.8889,  -6.8728,  -8.0373],\n",
       "         ...,\n",
       "         [ 15.8355, -26.4661, -26.1141,  ...,  -6.6704,  -6.8197,  -7.8724],\n",
       "         [ 15.8359, -26.6150, -26.2634,  ...,  -6.7503,  -6.9988,  -7.9506],\n",
       "         [ 15.7413, -27.0888, -26.7306,  ...,  -7.1496,  -7.4953,  -8.5754]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = 'train'\n",
    "q_id = 't1'\n",
    "\n",
    "wav_path = \"/ivi/ilps/personal/svakule/spoken_qa/gtts/annotated_wd_data_%s/wav/\" % split\n",
    "file = q_id + '.wav'\n",
    "\n",
    "speech, samplerate = sf.read(wav_path+file)\n",
    "input_values = processor(speech, return_tensors=\"pt\", padding=\"longest\").input_values\n",
    "input_values = input_values.to('cuda')\n",
    "logits = model(input_values).logits\n",
    "print(logits.shape)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
